{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2720e54b",
      "metadata": {
        "id": "2720e54b",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==2.8.0\n",
        "!pip install transformers==4.26\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install audiomentations\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install torchaudio\n",
        "!pip install tensorboardX\n",
        "!pip install accelerate -U\n",
        "!pip install hazm==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d65032-2f55-49b6-91bd-104bd73ef0cd",
      "metadata": {
        "id": "47d65032-2f55-49b6-91bd-104bd73ef0cd",
        "outputId": "fe689e54-7696-46a7-ac89-db476c726a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token <\"YOUR_HF_TOKEN\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9377f469",
      "metadata": {
        "id": "9377f469",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import hazm\n",
        "import string\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, load_metric, Dataset, concatenate_datasets, load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6d74e2-610f-49cf-a960-ae5dd36c11ca",
      "metadata": {
        "id": "3d6d74e2-610f-49cf-a960-ae5dd36c11ca"
      },
      "outputs": [],
      "source": [
        "_normalizer = hazm.Normalizer()\n",
        "\n",
        "chars_to_ignore = [\n",
        "    \",\", \"?\", \".\", \"!\", \"-\", \";\", \":\", '\"\"', \"%\", \"'\", '\"', \"�\",\n",
        "    \"#\", \"!\", \"؟\", \"?\", \"«\", \"»\", \"،\", \"(\", \")\", \"؛\", \"'ٔ\", \"٬\",'ٔ', \",\", \"?\",\n",
        "    \".\", \"!\", \"-\", \";\", \":\",'\"',\"“\", \"%\", \"‘\", \"”\", \"�\", \"–\", \"…\", \"_\", \"”\", '“', '„',\n",
        "    'ā', 'š',\n",
        "]\n",
        "\n",
        "chars_to_ignore = chars_to_ignore + list(string.ascii_lowercase + string.digits)\n",
        "\n",
        "chars_to_mapping = {\n",
        "    'ك': 'ک', 'دِ': 'د', 'بِ': 'ب', 'زِ': 'ز', 'ذِ': 'ذ', 'شِ': 'ش', 'سِ': 'س', 'ى': 'ی',\n",
        "    'ي': 'ی', 'أ': 'ا', 'ؤ': 'و', \"ے\": \"ی\", \"ۀ\": \"ه\", \"ﭘ\": \"پ\", \"ﮐ\": \"ک\", \"ﯽ\": \"ی\",\n",
        "    \"ﺎ\": \"ا\", \"ﺑ\": \"ب\", \"ﺘ\": \"ت\", \"ﺧ\": \"خ\", \"ﺩ\": \"د\", \"ﺱ\": \"س\", \"ﻀ\": \"ض\", \"ﻌ\": \"ع\",\n",
        "    \"ﻟ\": \"ل\", \"ﻡ\": \"م\", \"ﻢ\": \"م\", \"ﻪ\": \"ه\", \"ﻮ\": \"و\", 'ﺍ': \"ا\", 'ة': \"ه\",\n",
        "    'ﯾ': \"ی\", 'ﯿ': \"ی\", 'ﺒ': \"ب\", 'ﺖ': \"ت\", 'ﺪ': \"د\", 'ﺮ': \"ر\", 'ﺴ': \"س\", 'ﺷ': \"ش\",\n",
        "    'ﺸ': \"ش\", 'ﻋ': \"ع\", 'ﻤ': \"م\", 'ﻥ': \"ن\", 'ﻧ': \"ن\", 'ﻭ': \"و\", 'ﺭ': \"ر\", \"ﮔ\": \"گ\",\n",
        "    \"۱۴ام\": \"۱۴ ام\",\n",
        "\n",
        "    \"a\": \" ای \", \"b\": \" بی \", \"c\": \" سی \", \"d\": \" دی \", \"e\": \" ایی \", \"f\": \" اف \",\n",
        "    \"g\": \" جی \", \"h\": \" اچ \", \"i\": \" آی \", \"j\": \" جی \", \"k\": \" کی \", \"l\": \" ال \",\n",
        "    \"m\": \" ام \", \"n\": \" ان \", \"o\": \" او \", \"p\": \" پی \", \"q\": \" کیو \", \"r\": \" آر \",\n",
        "    \"s\": \" اس \", \"t\": \" تی \", \"u\": \" یو \", \"v\": \" وی \", \"w\": \" دبلیو \", \"x\": \" اکس \",\n",
        "    \"y\": \" وای \", \"z\": \" زد \",\n",
        "    \"\\u200c\": \" \", \"\\u200d\": \" \", \"\\u200e\": \" \", \"\\u200f\": \" \", \"\\ufeff\": \" \",\n",
        "}\n",
        "\n",
        "\n",
        "def multiple_replace(text, chars_to_mapping):\n",
        "    pattern = \"|\".join(map(re.escape, chars_to_mapping.keys()))\n",
        "    return re.sub(pattern, lambda m: chars_to_mapping[m.group()], str(text))\n",
        "\n",
        "def remove_special_characters(text, chars_to_ignore_regex):\n",
        "    text = re.sub(chars_to_ignore_regex, '', text).lower() + \" \"\n",
        "    return text\n",
        "\n",
        "def normalizer(row, chars_to_ignore=chars_to_ignore, chars_to_mapping=chars_to_mapping):\n",
        "    text = row['sentence']\n",
        "    chars_to_ignore_regex = f\"\"\"[{\"\".join(chars_to_ignore)}]\"\"\"\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    text = _normalizer.normalize(text)\n",
        "    text = multiple_replace(text, chars_to_mapping)\n",
        "    text = remove_special_characters(text, chars_to_ignore_regex)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "    _text = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            word = int(word)\n",
        "            _text.append(words(word))\n",
        "        except:\n",
        "            _text.append(word)\n",
        "\n",
        "    text = \" \".join(_text) + \" \"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not len(text) > 0:\n",
        "        return None\n",
        "\n",
        "    row['sentence'] = text\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82109dc1",
      "metadata": {
        "id": "82109dc1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"persian\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"persian\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6036a32f",
      "metadata": {
        "id": "6036a32f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "_common_voice_final = load_dataset(\"mohammadh128/common_voice_fa_preprocessed_and_augmented_training_and_evaluation_11_0\", num_proc=os.cpu_count())\n",
        "_common_voice_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90073a5",
      "metadata": {
        "id": "d90073a5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f47fa4",
      "metadata": {
        "id": "49f47fa4"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6381e77",
      "metadata": {
        "id": "a6381e77"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", use_cache = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32057f3",
      "metadata": {
        "id": "c32057f3"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a67bf33",
      "metadata": {
        "id": "3a67bf33"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"whisper_small-fa_v03\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=6.15044e-05,\n",
        "    warmup_steps=500,\n",
        "    max_steps=5000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    logging_steps=500,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6241828e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6241828e",
        "outputId": "4ef16d3d-158f-46a4-b01a-fa549585484c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=_common_voice_final[\"train\"],\n",
        "    eval_dataset=_common_voice_final[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6947a38b-5d92-4c0f-a184-8611fea94a5f",
      "metadata": {
        "id": "6947a38b-5d92-4c0f-a184-8611fea94a5f",
        "outputId": "5afda280-55c8-4e3e-d0b4-6df7f32106ca",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 53902\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 5000\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 15:56:37, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.210100</td>\n",
              "      <td>0.439317</td>\n",
              "      <td>44.170015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.717500</td>\n",
              "      <td>0.385981</td>\n",
              "      <td>40.532196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.585800</td>\n",
              "      <td>0.312391</td>\n",
              "      <td>35.520599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.508400</td>\n",
              "      <td>0.274010</td>\n",
              "      <td>31.008854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.443500</td>\n",
              "      <td>0.244815</td>\n",
              "      <td>29.795158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.392700</td>\n",
              "      <td>0.216328</td>\n",
              "      <td>27.243621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.340100</td>\n",
              "      <td>0.213681</td>\n",
              "      <td>26.007057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.236700</td>\n",
              "      <td>0.198893</td>\n",
              "      <td>28.516123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.212000</td>\n",
              "      <td>0.186622</td>\n",
              "      <td>25.889444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.183800</td>\n",
              "      <td>0.181340</td>\n",
              "      <td>23.145153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-500\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-500/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-500/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-500/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-1000\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-1000/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-1000/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-1000/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-1000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-1500\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-1500/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-1500/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-1500/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-1500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-2000\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-2000/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-2000/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-2000/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-2000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-2500\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-2500/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-2500/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-2500/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-2500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-3000\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-3000/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-3000/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-3000/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-3000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-3500\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-3500/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-3500/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-3500/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-3500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-4000\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-4000/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-4000/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-4000/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-4000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-4500\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-4500/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-4500/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-4500/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-4500/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10288\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to mohammadh128/whisper_small-fa_v03/checkpoint-5000\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-5000/config.json\n",
            "Configuration saved in mohammadh128/whisper_small-fa_v03/checkpoint-5000/generation_config.json\n",
            "Model weights saved in mohammadh128/whisper_small-fa_v03/checkpoint-5000/pytorch_model.bin\n",
            "Feature extractor saved in mohammadh128/whisper_small-fa_v03/checkpoint-5000/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.4830630645751953, metrics={'train_runtime': 57401.2749, 'train_samples_per_second': 1.394, 'train_steps_per_second': 0.087, 'total_flos': 2.308625485479936e+19, 'train_loss': 0.4830630645751953, 'epoch': 1.48})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "environment": {
      "kernel": "conda-env-pytorch-pytorch",
      "name": "workbench-notebooks.m111",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
    },
    "kernelspec": {
      "display_name": "PyTorch 1-13",
      "language": "python",
      "name": "conda-env-pytorch-pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
