{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IcPzNNH26U3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install openpyxl\n",
        "!pip install optuna\n",
        "!pip install ray[tune]\n",
        "!pip install wandb\n",
        "\n",
        "!pip install datasets==2.8.0\n",
        "!pip install transformers==4.26\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install audiomentations\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install torchaudio<0.12\n",
        "!pip install tensorboardX\n",
        "!pip install accelerate -U\n",
        "!pip install hazm==0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzETu8ht_MEu",
        "outputId": "3e62747b-2d1a-4c0d-d6e7-0c3adfb2e417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import hazm\n",
        "import string\n",
        "import os\n",
        "import ast\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from evaluate import load\n",
        "from tqdm.notebook import tqdm,trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets, load_metric, load_from_disk, DatasetDict\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=\"YOUR_WANDB_KEY\", relogin=True, force=True)\n",
        "os.environ['WANDB_PROJECT'] = \"hyperparameter_tuning_whisper_small_persian\"\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjVZqaqhOLTG"
      },
      "outputs": [],
      "source": [
        "_normalizer = hazm.Normalizer()\n",
        "\n",
        "chars_to_ignore = [\n",
        "    \",\", \"?\", \".\", \"!\", \"-\", \";\", \":\", '\"\"', \"%\", \"'\", '\"', \"�\",\n",
        "    \"#\", \"!\", \"؟\", \"?\", \"«\", \"»\", \"،\", \"(\", \")\", \"؛\", \"'ٔ\", \"٬\",'ٔ', \",\", \"?\",\n",
        "    \".\", \"!\", \"-\", \";\", \":\",'\"',\"“\", \"%\", \"‘\", \"”\", \"�\", \"–\", \"…\", \"_\", \"”\", '“', '„',\n",
        "    'ā', 'š',\n",
        "]\n",
        "\n",
        "chars_to_ignore = chars_to_ignore + list(string.ascii_lowercase + string.digits)\n",
        "\n",
        "chars_to_mapping = {\n",
        "    'ك': 'ک', 'دِ': 'د', 'بِ': 'ب', 'زِ': 'ز', 'ذِ': 'ذ', 'شِ': 'ش', 'سِ': 'س', 'ى': 'ی',\n",
        "    'ي': 'ی', 'أ': 'ا', 'ؤ': 'و', \"ے\": \"ی\", \"ۀ\": \"ه\", \"ﭘ\": \"پ\", \"ﮐ\": \"ک\", \"ﯽ\": \"ی\",\n",
        "    \"ﺎ\": \"ا\", \"ﺑ\": \"ب\", \"ﺘ\": \"ت\", \"ﺧ\": \"خ\", \"ﺩ\": \"د\", \"ﺱ\": \"س\", \"ﻀ\": \"ض\", \"ﻌ\": \"ع\",\n",
        "    \"ﻟ\": \"ل\", \"ﻡ\": \"م\", \"ﻢ\": \"م\", \"ﻪ\": \"ه\", \"ﻮ\": \"و\", 'ﺍ': \"ا\", 'ة': \"ه\",\n",
        "    'ﯾ': \"ی\", 'ﯿ': \"ی\", 'ﺒ': \"ب\", 'ﺖ': \"ت\", 'ﺪ': \"د\", 'ﺮ': \"ر\", 'ﺴ': \"س\", 'ﺷ': \"ش\",\n",
        "    'ﺸ': \"ش\", 'ﻋ': \"ع\", 'ﻤ': \"م\", 'ﻥ': \"ن\", 'ﻧ': \"ن\", 'ﻭ': \"و\", 'ﺭ': \"ر\", \"ﮔ\": \"گ\",\n",
        "    \"۱۴ام\": \"۱۴ ام\",\n",
        "\n",
        "    \"a\": \" ای \", \"b\": \" بی \", \"c\": \" سی \", \"d\": \" دی \", \"e\": \" ایی \", \"f\": \" اف \",\n",
        "    \"g\": \" جی \", \"h\": \" اچ \", \"i\": \" آی \", \"j\": \" جی \", \"k\": \" کی \", \"l\": \" ال \",\n",
        "    \"m\": \" ام \", \"n\": \" ان \", \"o\": \" او \", \"p\": \" پی \", \"q\": \" کیو \", \"r\": \" آر \",\n",
        "    \"s\": \" اس \", \"t\": \" تی \", \"u\": \" یو \", \"v\": \" وی \", \"w\": \" دبلیو \", \"x\": \" اکس \",\n",
        "    \"y\": \" وای \", \"z\": \" زد \",\n",
        "    \"\\u200c\": \" \", \"\\u200d\": \" \", \"\\u200e\": \" \", \"\\u200f\": \" \", \"\\ufeff\": \" \",\n",
        "}\n",
        "\n",
        "\n",
        "def multiple_replace(text, chars_to_mapping):\n",
        "    pattern = \"|\".join(map(re.escape, chars_to_mapping.keys()))\n",
        "    return re.sub(pattern, lambda m: chars_to_mapping[m.group()], str(text))\n",
        "\n",
        "def remove_special_characters(text, chars_to_ignore_regex):\n",
        "    text = re.sub(chars_to_ignore_regex, '', text).lower() + \" \"\n",
        "    return text\n",
        "\n",
        "def normalizer(row, chars_to_ignore=chars_to_ignore, chars_to_mapping=chars_to_mapping):\n",
        "    text = row['sentence']\n",
        "    chars_to_ignore_regex = f\"\"\"[{\"\".join(chars_to_ignore)}]\"\"\"\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    text = _normalizer.normalize(text)\n",
        "    text = multiple_replace(text, chars_to_mapping)\n",
        "    text = remove_special_characters(text, chars_to_ignore_regex)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "    _text = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            word = int(word)\n",
        "            _text.append(words(word))\n",
        "        except:\n",
        "            _text.append(word)\n",
        "\n",
        "    text = \" \".join(_text) + \" \"\n",
        "    text = text.strip()\n",
        "\n",
        "    if not len(text) > 0:\n",
        "        return None\n",
        "\n",
        "    row['sentence'] = text\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw9r93WBi9jQ"
      },
      "outputs": [],
      "source": [
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fa\", split=\"train\").shard(num_shards=10, index=0)\n",
        "common_voice[\"validation\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"fa\", split=\"validation\").shard(num_shards=10, index=0)\n",
        "\n",
        "common_voice = common_voice.map(normalizer)\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsHVzNRg_MEv",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"persian\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"persian\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mmEkeNvFybe"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWLIKiriFyY9",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsLYJrZ7FyWl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXFJz_TrGCLG"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIRKNKYZGPzx"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", use_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6-v13QQx-5P"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-fa\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=50,\n",
        "    max_steps=250,\n",
        "    eval_steps=250,\n",
        "    fp16=True,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    gradient_checkpointing=True,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    report_to=[\"wandb\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "yKMFqnLhOLTI",
        "outputId": "1a22b0c1-ab3f-4344-9565-453afe373921"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model_init=model_init,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FviCIxu3GvTP",
        "scrolled": true,
        "tags": [],
        "outputId": "d9e85be6-5ba9-45b1-f1fe-a17117c6ac7c",
        "colab": {
          "referenced_widgets": [
            "e1bf77c5231b4557b703d730d870be50",
            "574f737372534f1e924166b3d7f438d1",
            "2ad1d241f6ca4547889cc494e453662f",
            "bbaca207b30249caa697ed1d9ce2e269",
            "8cd79b388682488ebdbf945377ecb482",
            "fff7e1c517ab4ef9bbeaaf0aa454de25",
            "dbc0d4c1c14f4c5f901306cbf3fd1105",
            "2607375c1b954df1a9b140a9f770e224",
            "ae9f6fef5d674e8c9e4adb6217ad08a6",
            "b1b1c0c4b0194421ba4140354b9bf513",
            "8b9856d128f046c28facd5f58e6f2039",
            "2ee9b933f11d458684283eedb6844efb",
            "ddecd77cc32f43cdb2f0d7f2ed8d8831",
            "9270148fe9144ccb97f2161c6b77dc48",
            "55878a5fd4624539a779bf14aeba4163",
            "274b52ba4d40487eaefec3a7e81f64d1",
            "42918701d1554fb285f87a5b1ab67a79",
            "b6eda3a4db7d455a88def3e1e1153204",
            "4c4713afc6a64c27b3ec63344bc27460",
            "4a1a27e57c53470b9620c3783d1af34f",
            "47ffb3e4ec15411ea68521de23612ae1",
            "8e895180f9ee452c8c347557f9e61bf5",
            "42f7edfa96214fde98a32385b98867fc",
            "e0ace0d4f98d41309624e8ea86209d49",
            "faaf9d59510f47568e1bc9da9a9af569",
            "8238983d877e44baa228bc11161f401a",
            "72af15c5b4184dc2b1ea299e24d027f3",
            "43c6d864fa644690b5d1a0d2a677622a",
            "260b7bbf6d4a4b16965607b4a1bcca36",
            "396b90d793b04a6e86c815f6d72f3ec0",
            "79433eba22634d8b81554bf8d52a53d0",
            "e46d22bb22be43d0bba66fc93a294d35",
            "0a19ce282edc496d961b30c39c6e6c00",
            "4766edc8309f421fabb13afc62053750",
            "fe266a935f6b43ddb5fa89c7ad4b9125",
            "117805579af84de19d8e85e05020426f",
            "9e277514af6549189e866191bf83c85c",
            "31ed7b377607496ca7d21df373b5492b",
            "e6a5ea5f60b5468a962a48abe2220ae5",
            "55b1f60bed7440539517b015bd282d88",
            "aa12371065fb4df482a0e0db6b2a60a8",
            "e6561a4475954b26b313b86403c2aa6e",
            "5857d7ccda8c4aa2970e2874eeb3fd87",
            "3b1c56da8ee54a17acc24dd37cc61224",
            "3655001f1b694c2aa813c9d1db29ff9c",
            "c7e1fbdd56874a91a26a230e1da4c1b5",
            "75fee30cbfd14d60aa7e2f7324a5f536",
            "983853746b194ad6abdf706081962375",
            "f8a368cd5395496ba79559ecdfb29e61",
            "02805d8ecb2941a59736cf02decc73b2",
            "5d3f15b31bf7424ab38ced906ee4da15",
            "31607b79ead544feae453d056ebedc33",
            "ef97d930b81043c481cb58e226529842",
            "c860d3e2381a4e6482d3c48a5a5f35c2",
            "414371a36f3f410788a3c36fa609caa8",
            "9fbce3d1029f444eae016fa7d2f77e39",
            "18eb7c7012074e839027660b9e499e14",
            "9ffc7af3108041c98d72000a633cbff7",
            "b4f3edff756c45bfa9b7777434b5116a",
            "c0d5857c844846e99897b0cf003b36ec",
            "422ea1731dc44c378a889235bebc006c",
            "668ba360d53d48a49cf69f770c50f708",
            "24ade40133ce47d2b9e780ae113da6cd",
            "7d6631a572074c118ad8fc4a660d0c74",
            "2e41a9ef6cfa456c908657ae45ce6461",
            "e7f125d5829d403eae1a0529fc887ab3",
            "141266fa594840719945133dbe07f306",
            "78bcb4f2523148d59db67ad288f39a55",
            "6e033d6ba0bd4656ae959cb2742fe126",
            "4596292443314e55bb1a64e78c250d95",
            "9f930b8f0f7e407498a27f5447d33ffb",
            "90f51890f5a24300831572bd36faec6c",
            "8ed94f0189ff451c9c2304e823e7d36e",
            "835194619e1a4cceb3e7d5fb2cce8ce1",
            "3b8dc1b6163441258a93a0eb208b4027",
            "ffb1b4d575724d18991b3c2a427eaa41",
            "e0dae971271a4452b17c37dbf0baf8f0",
            "cdeaaf1c3be14a1299317e4840a49655",
            "ef1b97f105ab45619a339cd70150ceb2",
            "07e02e2cd17f4a2c889818290e308706",
            "46b9caeddf5b4f229f22d265444deddf",
            "2aeff73c91af4283b53a5579eed21a3f",
            "08931c17e3c94ff5bf3e8fc87241ffdd",
            "3bf47888318543edaaa2ba060edafcce",
            "df2e9c05337549bf90ee45da7326254b",
            "46cc64bc0c364e65ab70ea55b5abee5f",
            "636138407fac473a879d8f6e537529ae",
            "553803b78f1643a69862da3dfab734a2",
            "75e26f52093c47f0a28ae01844ffccd9",
            "39ce7cb7da904ecd9d02203cae082191",
            "d1b406d29f7c40b4a6659cac380cf3e1",
            "d8edc3eb14a74b4788bcb03eecb387ec",
            "3ba90ac10d734467b767eefb61f6be7a",
            "d175df76821947268a497fec06f9ec35",
            "c3f8c44714b6464bbe34f0fda7cd1881",
            "69d18abcca944172bcd288ea389dca17",
            "d18baac3e68c4584acfb194b834f6a4a",
            "144684f106d94cc4a68b4109d39e37be",
            "2941687390884e79b6256a144799036a",
            "2b7dce3966e14df1a4be9d251c49bd81",
            "52ef0e1eb6f4458ba36d22574bf1ae9d",
            "9612e63ed7a54b76848a833b4e9901ab",
            "d58ec90e1d094a04960765c8c829c0ca",
            "c07d419e6c644401b62523a593482442",
            "3c9c2d38e24046ceaf5222c2df4853ea",
            "ba721b75b2f7484f90b8b6e6b9ecd62b",
            "5539da0deeae4a6d98e1ec918b34ed90",
            "ebc8a4f454414f73b06b9fede74437e6",
            "751521b374b74c049696953f082fa4c1",
            "dbb78e82883c41cbab2933044c6e1665",
            "df2a0fd74877407e9bc01966b44898d4",
            "3c2da2b011d7448d8cb462a31bec135b",
            "f0425cd87051486dbd4e5e82ca1cedb4",
            "f633a616b5f5443d9df4610613c78462",
            "1d93b625aa154a5fb96fa1dbf7e45787",
            "d4118db1660a4bb7a854872c99d8cc8b",
            "01c580d242bf4d72ae17946716d2bcfb",
            "89600ebbaf4a46b6bdef3eccb9d403d7",
            "fcea2c49df394e16a8d7ba75c72fd830",
            "44162117d531444fa06a4534dc83d1f9",
            "6e0a3bfa560a45869b11e911ee7ce5de",
            "fcc0559546ba4ec58c6ab5b2b115163e",
            "6f6f85d360b54143b907c5204f15261e",
            "13ef515ecce8409993e1b526429f9342",
            "ee18e78f9bb44fac9543258210f571b2",
            "0faf7a82adf94091865329547dff0e2f",
            "9606a4e5ec2d46a68ae59e08c5a51923"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-09-19 11:31:19,665] A new study created in memory with name: no-name-f05b5f58-7cbd-48be-9871-4083c700c5ca\n",
            "Trial: {'learning_rate': 4.1669245403834655e-09, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammadh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_113122-lwwa90wb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lwwa90wb' target=\"_blank\">sweet-dream-1</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lwwa90wb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lwwa90wb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:53, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.105794</td>\n",
              "      <td>76.555024</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 11:39:27,778] Trial 0 finished with value: 76.55502392344498 and parameters: {'learning_rate': 4.1669245403834655e-09, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}. Best is trial 0 with value: 76.55502392344498.\n",
            "Trial: {'learning_rate': 9.688127253851351e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.10579</td></tr><tr><td>eval/runtime</td><td>47.0954</td></tr><tr><td>eval/samples_per_second</td><td>2.187</td></tr><tr><td>eval/steps_per_second</td><td>0.552</td></tr><tr><td>eval/wer</td><td>76.55502</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>4.04966</td></tr><tr><td>train/train_runtime</td><td>484.8575</td></tr><tr><td>train/train_samples_per_second</td><td>4.125</td></tr><tr><td>train/train_steps_per_second</td><td>0.516</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sweet-dream-1</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lwwa90wb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lwwa90wb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_113122-lwwa90wb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1bf77c5231b4557b703d730d870be50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113850788873, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_113936-ipzktn0s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ipzktn0s' target=\"_blank\">comic-galaxy-2</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ipzktn0s' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ipzktn0s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.514943</td>\n",
              "      <td>48.644338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 11:47:33,060] Trial 1 finished with value: 48.644338118022326 and parameters: {'learning_rate': 9.688127253851351e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}. Best is trial 1 with value: 48.644338118022326.\n",
            "Trial: {'learning_rate': 2.383279101856335e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.51494</td></tr><tr><td>eval/runtime</td><td>38.4496</td></tr><tr><td>eval/samples_per_second</td><td>2.679</td></tr><tr><td>eval/steps_per_second</td><td>0.676</td></tr><tr><td>eval/wer</td><td>48.64434</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.73688</td></tr><tr><td>train/train_runtime</td><td>482.357</td></tr><tr><td>train/train_samples_per_second</td><td>4.146</td></tr><tr><td>train/train_steps_per_second</td><td>0.518</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comic-galaxy-2</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ipzktn0s' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ipzktn0s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_113936-ipzktn0s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "574f737372534f1e924166b3d7f438d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113655844464018, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_114741-mhuhw5ne</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mhuhw5ne' target=\"_blank\">stellar-morning-3</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mhuhw5ne' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mhuhw5ne</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 16:27, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.058267</td>\n",
              "      <td>77.671451</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 12:04:23,408] Trial 2 finished with value: 77.67145135566189 and parameters: {'learning_rate': 2.383279101856335e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8}. Best is trial 1 with value: 48.644338118022326.\n",
            "Trial: {'learning_rate': 1.995805431634094e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ad1d241f6ca4547889cc494e453662f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.05827</td></tr><tr><td>eval/runtime</td><td>165.4024</td></tr><tr><td>eval/samples_per_second</td><td>0.623</td></tr><tr><td>eval/steps_per_second</td><td>0.079</td></tr><tr><td>eval/wer</td><td>77.67145</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>2.6524</td></tr><tr><td>train/train_runtime</td><td>1007.1652</td></tr><tr><td>train/train_samples_per_second</td><td>3.972</td></tr><tr><td>train/train_steps_per_second</td><td>0.248</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-morning-3</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mhuhw5ne' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mhuhw5ne</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_114741-mhuhw5ne/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbaca207b30249caa697ed1d9ce2e269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113344244464921, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_120430-9l1izaj1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/9l1izaj1' target=\"_blank\">lilac-silence-4</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/9l1izaj1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/9l1izaj1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:47, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.367596</td>\n",
              "      <td>75.598086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 12:19:31,842] Trial 3 finished with value: 75.5980861244019 and parameters: {'learning_rate': 1.995805431634094e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}. Best is trial 1 with value: 48.644338118022326.\n",
            "Trial: {'learning_rate': 3.4368244694542374e-09, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cd79b388682488ebdbf945377ecb482",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.036 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.049466…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.3676</td></tr><tr><td>eval/runtime</td><td>72.4227</td></tr><tr><td>eval/samples_per_second</td><td>1.422</td></tr><tr><td>eval/steps_per_second</td><td>0.718</td></tr><tr><td>eval/wer</td><td>75.59809</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>2.80232</td></tr><tr><td>train/train_runtime</td><td>905.5177</td></tr><tr><td>train/train_samples_per_second</td><td>4.417</td></tr><tr><td>train/train_steps_per_second</td><td>0.276</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-silence-4</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/9l1izaj1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/9l1izaj1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_120430-9l1izaj1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fff7e1c517ab4ef9bbeaaf0aa454de25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113601555553031, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_121938-2jkt3na1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2jkt3na1' target=\"_blank\">rural-night-5</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2jkt3na1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2jkt3na1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:19, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.274571</td>\n",
              "      <td>76.555024</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 12:34:11,678] Trial 4 finished with value: 76.55502392344498 and parameters: {'learning_rate': 3.4368244694542374e-09, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}. Best is trial 1 with value: 48.644338118022326.\n",
            "Trial: {'learning_rate': 2.0798908248250458e-08, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbc0d4c1c14f4c5f901306cbf3fd1105",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.27457</td></tr><tr><td>eval/runtime</td><td>45.9699</td></tr><tr><td>eval/samples_per_second</td><td>2.241</td></tr><tr><td>eval/steps_per_second</td><td>1.131</td></tr><tr><td>eval/wer</td><td>76.55502</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>4.02625</td></tr><tr><td>train/train_runtime</td><td>876.9891</td></tr><tr><td>train/train_samples_per_second</td><td>4.561</td></tr><tr><td>train/train_steps_per_second</td><td>0.285</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rural-night-5</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2jkt3na1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2jkt3na1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_121938-2jkt3na1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2607375c1b954df1a9b140a9f770e224",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112799322209968, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_123419-glk3wnpj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/glk3wnpj' target=\"_blank\">ethereal-cosmos-6</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/glk3wnpj' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/glk3wnpj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 04:48, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.976254</td>\n",
              "      <td>75.917065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 12:39:19,633] Trial 5 finished with value: 75.9170653907496 and parameters: {'learning_rate': 2.0798908248250458e-08, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 4}. Best is trial 1 with value: 48.644338118022326.\n",
            "Trial: {'learning_rate': 2.3193363256622462e-09, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae9f6fef5d674e8c9e4adb6217ad08a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>3.97625</td></tr><tr><td>eval/runtime</td><td>47.0972</td></tr><tr><td>eval/samples_per_second</td><td>2.187</td></tr><tr><td>eval/steps_per_second</td><td>0.552</td></tr><tr><td>eval/wer</td><td>75.91707</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.8858540032e+17</td></tr><tr><td>train/train_loss</td><td>4.12101</td></tr><tr><td>train/train_runtime</td><td>305.0119</td></tr><tr><td>train/train_samples_per_second</td><td>3.279</td></tr><tr><td>train/train_steps_per_second</td><td>0.82</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-cosmos-6</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/glk3wnpj' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/glk3wnpj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_123419-glk3wnpj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1b1c0c4b0194421ba4140354b9bf513",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112390599979942, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_123927-yetg55ms</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/yetg55ms' target=\"_blank\">curious-resonance-7</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/yetg55ms' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/yetg55ms</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:14, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.112364</td>\n",
              "      <td>76.395534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 12:53:55,261] Trial 6 pruned. \n",
            "Trial: {'learning_rate': 9.789082392396862e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b9856d128f046c28facd5f58e6f2039",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.035 MB of 0.035 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.11236</td></tr><tr><td>eval/runtime</td><td>46.7479</td></tr><tr><td>eval/samples_per_second</td><td>2.203</td></tr><tr><td>eval/steps_per_second</td><td>0.556</td></tr><tr><td>eval/wer</td><td>76.39553</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">curious-resonance-7</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/yetg55ms' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/yetg55ms</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_123927-yetg55ms/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ee9b933f11d458684283eedb6844efb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111262886673406, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_125401-fu284brd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fu284brd' target=\"_blank\">lucky-bee-8</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fu284brd' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fu284brd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 17:01, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.307825</td>\n",
              "      <td>111.004785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "[I 2023-09-19 13:11:16,857] Trial 7 pruned. \n",
            "Trial: {'learning_rate': 8.444275555012582e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.30783</td></tr><tr><td>eval/runtime</td><td>215.3849</td></tr><tr><td>eval/samples_per_second</td><td>0.478</td></tr><tr><td>eval/steps_per_second</td><td>0.06</td></tr><tr><td>eval/wer</td><td>111.00478</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lucky-bee-8</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fu284brd' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fu284brd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_125401-fu284brd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddecd77cc32f43cdb2f0d7f2ed8d8831",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113762100042853, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_131125-0k0hl0dp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0k0hl0dp' target=\"_blank\">solar-frog-9</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0k0hl0dp' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0k0hl0dp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:10, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.470439</td>\n",
              "      <td>48.006380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 13:25:50,023] Trial 8 finished with value: 48.006379585326954 and parameters: {'learning_rate': 8.444275555012582e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}. Best is trial 8 with value: 48.006379585326954.\n",
            "Trial: {'learning_rate': 6.403757601731923e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9270148fe9144ccb97f2161c6b77dc48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47044</td></tr><tr><td>eval/runtime</td><td>44.4699</td></tr><tr><td>eval/samples_per_second</td><td>2.316</td></tr><tr><td>eval/steps_per_second</td><td>1.169</td></tr><tr><td>eval/wer</td><td>48.00638</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>3.46995085344768e+18</td></tr><tr><td>train/train_loss</td><td>0.92513</td></tr><tr><td>train/train_runtime</td><td>870.3833</td></tr><tr><td>train/train_samples_per_second</td><td>4.596</td></tr><tr><td>train/train_steps_per_second</td><td>0.287</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-frog-9</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0k0hl0dp' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0k0hl0dp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_131125-0k0hl0dp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55878a5fd4624539a779bf14aeba4163",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113551144484922, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_132558-2wuvc4ix</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2wuvc4ix' target=\"_blank\">distinctive-dream-10</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2wuvc4ix' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2wuvc4ix</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.496899</td>\n",
              "      <td>49.282297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 13:33:53,711] Trial 9 finished with value: 49.282296650717704 and parameters: {'learning_rate': 6.403757601731923e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}. Best is trial 8 with value: 48.006379585326954.\n",
            "Trial: {'learning_rate': 6.501806437731933e-06, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "274b52ba4d40487eaefec3a7e81f64d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.4969</td></tr><tr><td>eval/runtime</td><td>38.0179</td></tr><tr><td>eval/samples_per_second</td><td>2.709</td></tr><tr><td>eval/steps_per_second</td><td>0.684</td></tr><tr><td>eval/wer</td><td>49.2823</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.7014</td></tr><tr><td>train/train_runtime</td><td>480.857</td></tr><tr><td>train/train_samples_per_second</td><td>4.159</td></tr><tr><td>train/train_steps_per_second</td><td>0.52</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">distinctive-dream-10</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2wuvc4ix' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2wuvc4ix</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_132558-2wuvc4ix/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42918701d1554fb285f87a5b1ab67a79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113413144468925, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_133400-116pwykg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/116pwykg' target=\"_blank\">daily-wave-11</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/116pwykg' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/116pwykg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 04:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.926725</td>\n",
              "      <td>59.968102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 13:39:08,830] Trial 10 finished with value: 59.96810207336522 and parameters: {'learning_rate': 6.501806437731933e-06, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2}. Best is trial 8 with value: 48.006379585326954.\n",
            "Trial: {'learning_rate': 8.369579425725039e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6eda3a4db7d455a88def3e1e1153204",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.92672</td></tr><tr><td>eval/runtime</td><td>55.4043</td></tr><tr><td>eval/samples_per_second</td><td>1.859</td></tr><tr><td>eval/steps_per_second</td><td>0.939</td></tr><tr><td>eval/wer</td><td>59.9681</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.8858540032e+17</td></tr><tr><td>train/train_loss</td><td>1.28672</td></tr><tr><td>train/train_runtime</td><td>312.278</td></tr><tr><td>train/train_samples_per_second</td><td>3.202</td></tr><tr><td>train/train_steps_per_second</td><td>0.801</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">daily-wave-11</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/116pwykg' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/116pwykg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_133400-116pwykg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c4713afc6a64c27b3ec63344bc27460",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113312199934928, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_133916-hp8dq52d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hp8dq52d' target=\"_blank\">expert-sun-12</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hp8dq52d' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hp8dq52d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.506295</td>\n",
              "      <td>45.933014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 13:47:16,172] Trial 11 finished with value: 45.933014354066984 and parameters: {'learning_rate': 8.369579425725039e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 11 with value: 45.933014354066984.\n",
            "Trial: {'learning_rate': 1.4376553055890788e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a1a27e57c53470b9620c3783d1af34f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.50629</td></tr><tr><td>eval/runtime</td><td>40.1847</td></tr><tr><td>eval/samples_per_second</td><td>2.563</td></tr><tr><td>eval/steps_per_second</td><td>1.294</td></tr><tr><td>eval/wer</td><td>45.93301</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.71963</td></tr><tr><td>train/train_runtime</td><td>484.6114</td></tr><tr><td>train/train_samples_per_second</td><td>4.127</td></tr><tr><td>train/train_steps_per_second</td><td>0.516</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">expert-sun-12</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hp8dq52d' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hp8dq52d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_133916-hp8dq52d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47ffb3e4ec15411ea68521de23612ae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112750466660751, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_134723-4dy2fqre</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/4dy2fqre' target=\"_blank\">peach-oath-13</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/4dy2fqre' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/4dy2fqre</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:45, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.478225</td>\n",
              "      <td>48.803828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 13:55:20,404] Trial 12 finished with value: 48.803827751196174 and parameters: {'learning_rate': 1.4376553055890788e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 11 with value: 45.933014354066984.\n",
            "Trial: {'learning_rate': 1.079348891152709e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e895180f9ee452c8c347557f9e61bf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47822</td></tr><tr><td>eval/runtime</td><td>39.6256</td></tr><tr><td>eval/samples_per_second</td><td>2.599</td></tr><tr><td>eval/steps_per_second</td><td>1.312</td></tr><tr><td>eval/wer</td><td>48.80383</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.85487</td></tr><tr><td>train/train_runtime</td><td>481.409</td></tr><tr><td>train/train_samples_per_second</td><td>4.154</td></tr><tr><td>train/train_steps_per_second</td><td>0.519</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peach-oath-13</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/4dy2fqre' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/4dy2fqre</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_134723-4dy2fqre/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42f7edfa96214fde98a32385b98867fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113869011104624, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_135527-t9kdfu1d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t9kdfu1d' target=\"_blank\">misty-snow-14</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t9kdfu1d' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t9kdfu1d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.501642</td>\n",
              "      <td>51.834131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 14:03:24,400] Trial 13 finished with value: 51.8341307814992 and parameters: {'learning_rate': 1.079348891152709e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 11 with value: 45.933014354066984.\n",
            "Trial: {'learning_rate': 2.3582410357908704e-06, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0ace0d4f98d41309624e8ea86209d49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.50164</td></tr><tr><td>eval/runtime</td><td>39.6432</td></tr><tr><td>eval/samples_per_second</td><td>2.598</td></tr><tr><td>eval/steps_per_second</td><td>1.312</td></tr><tr><td>eval/wer</td><td>51.83413</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.95829</td></tr><tr><td>train/train_runtime</td><td>481.1941</td></tr><tr><td>train/train_samples_per_second</td><td>4.156</td></tr><tr><td>train/train_steps_per_second</td><td>0.52</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misty-snow-14</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t9kdfu1d' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t9kdfu1d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_135527-t9kdfu1d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faaf9d59510f47568e1bc9da9a9af569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112724333280413, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_140331-acvwzmjm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/acvwzmjm' target=\"_blank\">devout-blaze-15</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/acvwzmjm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/acvwzmjm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 06:36, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.236982</td>\n",
              "      <td>114.035088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "[I 2023-09-19 14:10:19,260] Trial 14 pruned. \n",
            "Trial: {'learning_rate': 3.652544004566609e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.23698</td></tr><tr><td>eval/runtime</td><td>155.2711</td></tr><tr><td>eval/samples_per_second</td><td>0.663</td></tr><tr><td>eval/steps_per_second</td><td>0.335</td></tr><tr><td>eval/wer</td><td>114.03509</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-blaze-15</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/acvwzmjm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/acvwzmjm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_140331-acvwzmjm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8238983d877e44baa228bc11161f401a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111260322221723, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_141027-xrvgg5j2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xrvgg5j2' target=\"_blank\">confused-wildflower-16</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xrvgg5j2' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xrvgg5j2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:04, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.404841</td>\n",
              "      <td>44.816587</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 14:24:46,422] Trial 15 finished with value: 44.81658692185008 and parameters: {'learning_rate': 3.652544004566609e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 3.770070885800355e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72af15c5b4184dc2b1ea299e24d027f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40484</td></tr><tr><td>eval/runtime</td><td>39.8581</td></tr><tr><td>eval/samples_per_second</td><td>2.584</td></tr><tr><td>eval/steps_per_second</td><td>1.305</td></tr><tr><td>eval/wer</td><td>44.81659</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.44523568480256e+18</td></tr><tr><td>train/train_loss</td><td>0.56318</td></tr><tr><td>train/train_runtime</td><td>864.3287</td></tr><tr><td>train/train_samples_per_second</td><td>4.628</td></tr><tr><td>train/train_steps_per_second</td><td>0.289</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">confused-wildflower-16</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xrvgg5j2' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xrvgg5j2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_141027-xrvgg5j2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43c6d864fa644690b5d1a0d2a677622a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112809866608587, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_142454-mgup6das</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mgup6das' target=\"_blank\">dry-valley-17</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mgup6das' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mgup6das</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:43, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.476966</td>\n",
              "      <td>46.730463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 14:32:50,590] Trial 16 finished with value: 46.730462519936204 and parameters: {'learning_rate': 3.770070885800355e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 9.817395467598084e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47697</td></tr><tr><td>eval/runtime</td><td>39.1029</td></tr><tr><td>eval/samples_per_second</td><td>2.634</td></tr><tr><td>eval/steps_per_second</td><td>0.332</td></tr><tr><td>eval/wer</td><td>46.73046</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.70235</td></tr><tr><td>train/train_runtime</td><td>481.3146</td></tr><tr><td>train/train_samples_per_second</td><td>4.155</td></tr><tr><td>train/train_steps_per_second</td><td>0.519</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dry-valley-17</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mgup6das' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/mgup6das</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_142454-mgup6das/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "260b7bbf6d4a4b16965607b4a1bcca36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113763033386527, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_143300-n6uevzf0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/n6uevzf0' target=\"_blank\">amber-rain-18</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/n6uevzf0' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/n6uevzf0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:45, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.505528</td>\n",
              "      <td>49.601276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 14:40:58,779] Trial 17 finished with value: 49.601275917065394 and parameters: {'learning_rate': 9.817395467598084e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 2.6767681174379726e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "396b90d793b04a6e86c815f6d72f3ec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.50553</td></tr><tr><td>eval/runtime</td><td>39.6321</td></tr><tr><td>eval/samples_per_second</td><td>2.599</td></tr><tr><td>eval/steps_per_second</td><td>1.312</td></tr><tr><td>eval/wer</td><td>49.60128</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.73292</td></tr><tr><td>train/train_runtime</td><td>485.3506</td></tr><tr><td>train/train_samples_per_second</td><td>4.121</td></tr><tr><td>train/train_steps_per_second</td><td>0.515</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">amber-rain-18</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/n6uevzf0' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/n6uevzf0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_143300-n6uevzf0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79433eba22634d8b81554bf8d52a53d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112676633314954, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_144106-8qgowejh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8qgowejh' target=\"_blank\">soft-blaze-19</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8qgowejh' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8qgowejh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:06, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.406766</td>\n",
              "      <td>51.834131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "[I 2023-09-19 14:55:26,771] Trial 18 pruned. \n",
            "Trial: {'learning_rate': 2.98508091976232e-06, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46d22bb22be43d0bba66fc93a294d35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40677</td></tr><tr><td>eval/runtime</td><td>44.2202</td></tr><tr><td>eval/samples_per_second</td><td>2.329</td></tr><tr><td>eval/steps_per_second</td><td>1.176</td></tr><tr><td>eval/wer</td><td>51.83413</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-blaze-19</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8qgowejh' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8qgowejh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_144106-8qgowejh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a19ce282edc496d961b30c39c6e6c00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112639622297138, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_145535-e0spzp5c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e0spzp5c' target=\"_blank\">eager-sea-20</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e0spzp5c' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e0spzp5c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 06:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.175028</td>\n",
              "      <td>108.612440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "[I 2023-09-19 15:01:53,574] Trial 19 pruned. \n",
            "Trial: {'learning_rate': 2.1072836130896732e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4766edc8309f421fabb13afc62053750",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.035 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.050512…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.17503</td></tr><tr><td>eval/runtime</td><td>124.5956</td></tr><tr><td>eval/samples_per_second</td><td>0.827</td></tr><tr><td>eval/steps_per_second</td><td>0.417</td></tr><tr><td>eval/wer</td><td>108.61244</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-sea-20</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e0spzp5c' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e0spzp5c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_145535-e0spzp5c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe266a935f6b43ddb5fa89c7ad4b9125",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113433466622762, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_150200-gc0rm4bh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/gc0rm4bh' target=\"_blank\">dazzling-galaxy-21</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/gc0rm4bh' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/gc0rm4bh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:22, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.409981</td>\n",
              "      <td>51.355662</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "[I 2023-09-19 15:16:37,345] Trial 20 pruned. \n",
            "Trial: {'learning_rate': 3.0451651641511873e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "117805579af84de19d8e85e05020426f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40998</td></tr><tr><td>eval/runtime</td><td>53.4939</td></tr><tr><td>eval/samples_per_second</td><td>1.925</td></tr><tr><td>eval/steps_per_second</td><td>0.243</td></tr><tr><td>eval/wer</td><td>51.35566</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-galaxy-21</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/gc0rm4bh' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/gc0rm4bh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_150200-gc0rm4bh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e277514af6549189e866191bf83c85c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112543088933712, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_151646-7bvbecec</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7bvbecec' target=\"_blank\">crimson-shadow-22</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7bvbecec' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7bvbecec</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.469806</td>\n",
              "      <td>46.570973</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 15:24:45,322] Trial 21 finished with value: 46.57097288676236 and parameters: {'learning_rate': 3.0451651641511873e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 2.7605274602805756e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31ed7b377607496ca7d21df373b5492b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.46981</td></tr><tr><td>eval/runtime</td><td>39.6171</td></tr><tr><td>eval/samples_per_second</td><td>2.6</td></tr><tr><td>eval/steps_per_second</td><td>0.328</td></tr><tr><td>eval/wer</td><td>46.57097</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>3.17905676992512e+18</td></tr><tr><td>train/train_loss</td><td>0.7198</td></tr><tr><td>train/train_runtime</td><td>485.1046</td></tr><tr><td>train/train_samples_per_second</td><td>4.123</td></tr><tr><td>train/train_steps_per_second</td><td>0.515</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crimson-shadow-22</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7bvbecec' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7bvbecec</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_151646-7bvbecec/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6a5ea5f60b5468a962a48abe2220ae5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113653244521831, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_152452-ga33zfok</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ga33zfok' target=\"_blank\">astral-monkey-23</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ga33zfok' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ga33zfok</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:43, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.470714</td>\n",
              "      <td>46.889952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 15:32:51,614] Trial 22 finished with value: 46.889952153110045 and parameters: {'learning_rate': 2.7605274602805756e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 4.4221160928461253e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47071</td></tr><tr><td>eval/runtime</td><td>39.1756</td></tr><tr><td>eval/samples_per_second</td><td>2.629</td></tr><tr><td>eval/steps_per_second</td><td>0.332</td></tr><tr><td>eval/wer</td><td>46.88995</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.73148</td></tr><tr><td>train/train_runtime</td><td>483.4803</td></tr><tr><td>train/train_samples_per_second</td><td>4.137</td></tr><tr><td>train/train_steps_per_second</td><td>0.517</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">astral-monkey-23</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ga33zfok' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ga33zfok</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_152452-ga33zfok/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55b1f60bed7440539517b015bd282d88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112577966640756, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_153259-e7qdmeuv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e7qdmeuv' target=\"_blank\">rose-yogurt-24</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e7qdmeuv' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e7qdmeuv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:45, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.483872</td>\n",
              "      <td>47.527911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 15:40:57,966] Trial 23 finished with value: 47.52791068580542 and parameters: {'learning_rate': 4.4221160928461253e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 5.167365174356365e-06, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa12371065fb4df482a0e0db6b2a60a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.48387</td></tr><tr><td>eval/runtime</td><td>39.4742</td></tr><tr><td>eval/samples_per_second</td><td>2.609</td></tr><tr><td>eval/steps_per_second</td><td>0.329</td></tr><tr><td>eval/wer</td><td>47.52791</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.69616</td></tr><tr><td>train/train_runtime</td><td>483.4858</td></tr><tr><td>train/train_samples_per_second</td><td>4.137</td></tr><tr><td>train/train_steps_per_second</td><td>0.517</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rose-yogurt-24</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e7qdmeuv' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e7qdmeuv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_153259-e7qdmeuv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6561a4475954b26b313b86403c2aa6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112751977796305, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_154105-jbvrevl8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jbvrevl8' target=\"_blank\">woven-sunset-25</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jbvrevl8' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jbvrevl8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 08:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.913611</td>\n",
              "      <td>63.157895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "[I 2023-09-19 15:49:57,350] Trial 24 pruned. \n",
            "Trial: {'learning_rate': 9.705301864080131e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5857d7ccda8c4aa2970e2874eeb3fd87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.91361</td></tr><tr><td>eval/runtime</td><td>95.8504</td></tr><tr><td>eval/samples_per_second</td><td>1.075</td></tr><tr><td>eval/steps_per_second</td><td>0.136</td></tr><tr><td>eval/wer</td><td>63.15789</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-sunset-25</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jbvrevl8' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jbvrevl8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_154105-jbvrevl8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b1c56da8ee54a17acc24dd37cc61224",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112662766794932, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_155004-2r4fihdt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2r4fihdt' target=\"_blank\">serene-hill-26</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2r4fihdt' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2r4fihdt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.509393</td>\n",
              "      <td>47.368421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 15:57:59,682] Trial 25 finished with value: 47.368421052631575 and parameters: {'learning_rate': 9.705301864080131e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 1.6578113816176173e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3655001f1b694c2aa813c9d1db29ff9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.50939</td></tr><tr><td>eval/runtime</td><td>39.5698</td></tr><tr><td>eval/samples_per_second</td><td>2.603</td></tr><tr><td>eval/steps_per_second</td><td>1.314</td></tr><tr><td>eval/wer</td><td>47.36842</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15434160128e+18</td></tr><tr><td>train/train_loss</td><td>0.73991</td></tr><tr><td>train/train_runtime</td><td>479.5181</td></tr><tr><td>train/train_samples_per_second</td><td>4.171</td></tr><tr><td>train/train_steps_per_second</td><td>0.521</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">serene-hill-26</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2r4fihdt' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/2r4fihdt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_155004-2r4fihdt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7e1fbdd56874a91a26a230e1da4c1b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113266522200623, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_155807-ex641nud</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ex641nud' target=\"_blank\">eternal-snowflake-27</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ex641nud' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ex641nud</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:43, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.478763</td>\n",
              "      <td>48.325359</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 16:06:04,280] Trial 26 finished with value: 48.32535885167464 and parameters: {'learning_rate': 1.6578113816176173e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 3.428626313374168e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75fee30cbfd14d60aa7e2f7324a5f536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47876</td></tr><tr><td>eval/runtime</td><td>39.2742</td></tr><tr><td>eval/samples_per_second</td><td>2.623</td></tr><tr><td>eval/steps_per_second</td><td>0.331</td></tr><tr><td>eval/wer</td><td>48.32536</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.817</td></tr><tr><td>train/train_runtime</td><td>481.5682</td></tr><tr><td>train/train_samples_per_second</td><td>4.153</td></tr><tr><td>train/train_steps_per_second</td><td>0.519</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eternal-snowflake-27</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ex641nud' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ex641nud</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_155807-ex641nud/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "983853746b194ad6abdf706081962375",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113496777802033, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_160612-kpbysid9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kpbysid9' target=\"_blank\">deft-frog-28</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kpbysid9' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kpbysid9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.462730</td>\n",
              "      <td>47.846890</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 16:14:10,725] Trial 27 finished with value: 47.84688995215311 and parameters: {'learning_rate': 3.428626313374168e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 2}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 1.3723683077523056e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8a368cd5395496ba79559ecdfb29e61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.46273</td></tr><tr><td>eval/runtime</td><td>40.0064</td></tr><tr><td>eval/samples_per_second</td><td>2.575</td></tr><tr><td>eval/steps_per_second</td><td>1.3</td></tr><tr><td>eval/wer</td><td>47.84689</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.71019</td></tr><tr><td>train/train_runtime</td><td>483.5545</td></tr><tr><td>train/train_samples_per_second</td><td>4.136</td></tr><tr><td>train/train_steps_per_second</td><td>0.517</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-frog-28</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kpbysid9' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kpbysid9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_160612-kpbysid9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02805d8ecb2941a59736cf02decc73b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112654255541404, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_161418-vo3yf4ms</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vo3yf4ms' target=\"_blank\">swept-breeze-29</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vo3yf4ms' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vo3yf4ms</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:24, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.431760</td>\n",
              "      <td>53.748006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "[I 2023-09-19 16:28:56,919] Trial 28 pruned. \n",
            "Trial: {'learning_rate': 4.7022294893146995e-05, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.43176</td></tr><tr><td>eval/runtime</td><td>44.9633</td></tr><tr><td>eval/samples_per_second</td><td>2.291</td></tr><tr><td>eval/steps_per_second</td><td>1.156</td></tr><tr><td>eval/wer</td><td>53.74801</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-breeze-29</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vo3yf4ms' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vo3yf4ms</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_161418-vo3yf4ms/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d3f15b31bf7424ab38ced906ee4da15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113484866736042, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_162905-e2kbd29z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e2kbd29z' target=\"_blank\">super-gorge-30</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e2kbd29z' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e2kbd29z</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 04:40, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.533927</td>\n",
              "      <td>51.196172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "[I 2023-09-19 16:33:57,389] Trial 29 pruned. \n",
            "Trial: {'learning_rate': 1.312127998506964e-06, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31607b79ead544feae453d056ebedc33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.53393</td></tr><tr><td>eval/runtime</td><td>39.4721</td></tr><tr><td>eval/samples_per_second</td><td>2.609</td></tr><tr><td>eval/steps_per_second</td><td>0.329</td></tr><tr><td>eval/wer</td><td>51.19617</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">super-gorge-30</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e2kbd29z' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/e2kbd29z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_162905-e2kbd29z/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef97d930b81043c481cb58e226529842",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113429022306163, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_163409-bskurgbe</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bskurgbe' target=\"_blank\">eternal-disco-31</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bskurgbe' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bskurgbe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 10:23, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.305322</td>\n",
              "      <td>102.870813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 16:44:44,783] Trial 30 pruned. \n",
            "Trial: {'learning_rate': 4.414112030585282e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c860d3e2381a4e6482d3c48a5a5f35c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.30532</td></tr><tr><td>eval/runtime</td><td>194.1619</td></tr><tr><td>eval/samples_per_second</td><td>0.53</td></tr><tr><td>eval/steps_per_second</td><td>0.134</td></tr><tr><td>eval/wer</td><td>102.87081</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eternal-disco-31</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bskurgbe' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bskurgbe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_163409-bskurgbe/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "414371a36f3f410788a3c36fa609caa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113776866598831, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_164452-8vxntt3k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8vxntt3k' target=\"_blank\">swept-cosmos-32</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8vxntt3k' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8vxntt3k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.497850</td>\n",
              "      <td>48.484848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 16:52:54,948] Trial 31 finished with value: 48.484848484848484 and parameters: {'learning_rate': 4.414112030585282e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 9.805008535253484e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fbce3d1029f444eae016fa7d2f77e39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.49785</td></tr><tr><td>eval/runtime</td><td>39.3371</td></tr><tr><td>eval/samples_per_second</td><td>2.618</td></tr><tr><td>eval/steps_per_second</td><td>0.33</td></tr><tr><td>eval/wer</td><td>48.48485</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.59957728608256e+18</td></tr><tr><td>train/train_loss</td><td>0.6999</td></tr><tr><td>train/train_runtime</td><td>487.3592</td></tr><tr><td>train/train_samples_per_second</td><td>4.104</td></tr><tr><td>train/train_steps_per_second</td><td>0.513</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-cosmos-32</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8vxntt3k' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/8vxntt3k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_164452-8vxntt3k/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18eb7c7012074e839027660b9e499e14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111301349996615, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_165303-7591zq2s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7591zq2s' target=\"_blank\">wise-mountain-33</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7591zq2s' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7591zq2s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.520562</td>\n",
              "      <td>49.122807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "[I 2023-09-19 17:01:04,789] Trial 32 pruned. \n",
            "Trial: {'learning_rate': 2.6683621924709037e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ffc7af3108041c98d72000a633cbff7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.52056</td></tr><tr><td>eval/runtime</td><td>39.8231</td></tr><tr><td>eval/samples_per_second</td><td>2.586</td></tr><tr><td>eval/steps_per_second</td><td>0.326</td></tr><tr><td>eval/wer</td><td>49.12281</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wise-mountain-33</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7591zq2s' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/7591zq2s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_165303-7591zq2s/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4f3edff756c45bfa9b7777434b5116a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113625066678246, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_170112-lrmjn15f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lrmjn15f' target=\"_blank\">dandy-resonance-34</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lrmjn15f' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lrmjn15f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:54, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.468642</td>\n",
              "      <td>47.208931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 17:09:19,011] Trial 33 finished with value: 47.208931419457734 and parameters: {'learning_rate': 2.6683621924709037e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 5.74845963342417e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d5857c844846e99897b0cf003b36ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.46864</td></tr><tr><td>eval/runtime</td><td>39.8553</td></tr><tr><td>eval/samples_per_second</td><td>2.584</td></tr><tr><td>eval/steps_per_second</td><td>0.326</td></tr><tr><td>eval/wer</td><td>47.20893</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15434160128e+18</td></tr><tr><td>train/train_loss</td><td>0.73459</td></tr><tr><td>train/train_runtime</td><td>491.4166</td></tr><tr><td>train/train_samples_per_second</td><td>4.07</td></tr><tr><td>train/train_steps_per_second</td><td>0.509</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dandy-resonance-34</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lrmjn15f' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/lrmjn15f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_170112-lrmjn15f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "422ea1731dc44c378a889235bebc006c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112672277765039, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_170927-bhqc4n1c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bhqc4n1c' target=\"_blank\">glad-pond-35</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bhqc4n1c' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bhqc4n1c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:54, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.489742</td>\n",
              "      <td>46.411483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 17:17:34,198] Trial 34 finished with value: 46.411483253588514 and parameters: {'learning_rate': 5.74845963342417e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 5.9270370038349626e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "668ba360d53d48a49cf69f770c50f708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.48974</td></tr><tr><td>eval/runtime</td><td>39.9393</td></tr><tr><td>eval/samples_per_second</td><td>2.579</td></tr><tr><td>eval/steps_per_second</td><td>0.325</td></tr><tr><td>eval/wer</td><td>46.41148</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.70015</td></tr><tr><td>train/train_runtime</td><td>492.2007</td></tr><tr><td>train/train_samples_per_second</td><td>4.063</td></tr><tr><td>train/train_steps_per_second</td><td>0.508</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glad-pond-35</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bhqc4n1c' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bhqc4n1c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_170927-bhqc4n1c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24ade40133ce47d2b9e780ae113da6cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113787199913834, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_171741-3uli8qhe</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3uli8qhe' target=\"_blank\">glamorous-voice-36</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3uli8qhe' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3uli8qhe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 07:54, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.482425</td>\n",
              "      <td>46.251994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 17:25:48,314] Trial 35 finished with value: 46.25199362041467 and parameters: {'learning_rate': 5.9270370038349626e-05, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8}. Best is trial 15 with value: 44.81658692185008.\n",
            "Trial: {'learning_rate': 6.150443574387932e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d6631a572074c118ad8fc4a660d0c74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.48242</td></tr><tr><td>eval/runtime</td><td>40.1664</td></tr><tr><td>eval/samples_per_second</td><td>2.564</td></tr><tr><td>eval/steps_per_second</td><td>0.324</td></tr><tr><td>eval/wer</td><td>46.25199</td></tr><tr><td>train/epoch</td><td>0.74</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>5.7717080064e+17</td></tr><tr><td>train/train_loss</td><td>0.69819</td></tr><tr><td>train/train_runtime</td><td>491.3334</td></tr><tr><td>train/train_samples_per_second</td><td>4.071</td></tr><tr><td>train/train_steps_per_second</td><td>0.509</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glamorous-voice-36</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3uli8qhe' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3uli8qhe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_171741-3uli8qhe/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e41a9ef6cfa456c908657ae45ce6461",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111277481120649, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_172555-1r6920ej</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/1r6920ej' target=\"_blank\">leafy-universe-37</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/1r6920ej' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/1r6920ej</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:15, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.399850</td>\n",
              "      <td>39.712919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 17:40:26,072] Trial 36 finished with value: 39.71291866028708 and parameters: {'learning_rate': 6.150443574387932e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 1.0654085371486253e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7f125d5829d403eae1a0529fc887ab3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.030 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.059345…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.39985</td></tr><tr><td>eval/runtime</td><td>38.595</td></tr><tr><td>eval/samples_per_second</td><td>2.669</td></tr><tr><td>eval/steps_per_second</td><td>0.674</td></tr><tr><td>eval/wer</td><td>39.71292</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53859</td></tr><tr><td>train/train_runtime</td><td>874.7416</td></tr><tr><td>train/train_samples_per_second</td><td>4.573</td></tr><tr><td>train/train_steps_per_second</td><td>0.286</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">leafy-universe-37</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/1r6920ej' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/1r6920ej</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_172555-1r6920ej/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141266fa594840719945133dbe07f306",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113715088868048, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_174032-g1ylwsq4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/g1ylwsq4' target=\"_blank\">copper-pond-38</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/g1ylwsq4' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/g1ylwsq4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:17, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.447890</td>\n",
              "      <td>45.295056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 17:55:03,797] Trial 37 finished with value: 45.29505582137161 and parameters: {'learning_rate': 1.0654085371486253e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 9.791946322403728e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.44789</td></tr><tr><td>eval/runtime</td><td>38.4825</td></tr><tr><td>eval/samples_per_second</td><td>2.677</td></tr><tr><td>eval/steps_per_second</td><td>0.676</td></tr><tr><td>eval/wer</td><td>45.29506</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.81723</td></tr><tr><td>train/train_runtime</td><td>874.9661</td></tr><tr><td>train/train_samples_per_second</td><td>4.572</td></tr><tr><td>train/train_steps_per_second</td><td>0.286</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">copper-pond-38</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/g1ylwsq4' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/g1ylwsq4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_174032-g1ylwsq4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78bcb4f2523148d59db67ad288f39a55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111257874435978, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_175513-0i95lb6j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0i95lb6j' target=\"_blank\">fast-cloud-39</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0i95lb6j' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0i95lb6j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:25, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.496428</td>\n",
              "      <td>46.251994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 18:09:52,819] Trial 38 finished with value: 46.25199362041467 and parameters: {'learning_rate': 9.791946322403728e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 5.6435457754185605e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e033d6ba0bd4656ae959cb2742fe126",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.49643</td></tr><tr><td>eval/runtime</td><td>45.8916</td></tr><tr><td>eval/samples_per_second</td><td>2.244</td></tr><tr><td>eval/steps_per_second</td><td>0.567</td></tr><tr><td>eval/wer</td><td>46.25199</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.85633</td></tr><tr><td>train/train_runtime</td><td>885.9745</td></tr><tr><td>train/train_samples_per_second</td><td>4.515</td></tr><tr><td>train/train_steps_per_second</td><td>0.282</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fast-cloud-39</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0i95lb6j' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0i95lb6j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_175513-0i95lb6j/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4596292443314e55bb1a64e78c250d95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111332895557603, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_181000-xsy2dqea</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xsy2dqea' target=\"_blank\">brisk-flower-40</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xsy2dqea' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xsy2dqea</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:25, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.797800</td>\n",
              "      <td>51.515152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 18:24:40,973] Trial 39 pruned. \n",
            "Trial: {'learning_rate': 2.1471854116283815e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.7978</td></tr><tr><td>eval/runtime</td><td>38.8032</td></tr><tr><td>eval/samples_per_second</td><td>2.654</td></tr><tr><td>eval/steps_per_second</td><td>0.67</td></tr><tr><td>eval/wer</td><td>51.51515</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">brisk-flower-40</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xsy2dqea' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xsy2dqea</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_181000-xsy2dqea/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f930b8f0f7e407498a27f5447d33ffb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111373878892563, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_182450-12lp0loc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/12lp0loc' target=\"_blank\">genial-aardvark-41</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/12lp0loc' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/12lp0loc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:32, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.414465</td>\n",
              "      <td>51.515152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 18:39:37,403] Trial 40 pruned. \n",
            "Trial: {'learning_rate': 6.801926347113953e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f51890f5a24300831572bd36faec6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41446</td></tr><tr><td>eval/runtime</td><td>45.8573</td></tr><tr><td>eval/samples_per_second</td><td>2.246</td></tr><tr><td>eval/steps_per_second</td><td>0.567</td></tr><tr><td>eval/wer</td><td>51.51515</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">genial-aardvark-41</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/12lp0loc' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/12lp0loc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_182450-12lp0loc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ed94f0189ff451c9c2304e823e7d36e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112844666543727, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_183945-3rljdki9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3rljdki9' target=\"_blank\">pious-plasma-42</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3rljdki9' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3rljdki9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:33, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.411479</td>\n",
              "      <td>50.717703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 18:54:33,277] Trial 41 pruned. \n",
            "Trial: {'learning_rate': 6.266295321357901e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "835194619e1a4cceb3e7d5fb2cce8ce1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41148</td></tr><tr><td>eval/runtime</td><td>45.9222</td></tr><tr><td>eval/samples_per_second</td><td>2.243</td></tr><tr><td>eval/steps_per_second</td><td>0.566</td></tr><tr><td>eval/wer</td><td>50.7177</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pious-plasma-42</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3rljdki9' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3rljdki9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_183945-3rljdki9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b8dc1b6163441258a93a0eb208b4027",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113529855437163, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_185440-207uwibk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/207uwibk' target=\"_blank\">splendid-cherry-43</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/207uwibk' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/207uwibk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:26, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.401638</td>\n",
              "      <td>41.786284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 19:09:20,709] Trial 42 finished with value: 41.78628389154705 and parameters: {'learning_rate': 6.266295321357901e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 1.6979784502143475e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffb1b4d575724d18991b3c2a427eaa41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.036 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.049430…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40164</td></tr><tr><td>eval/runtime</td><td>38.5122</td></tr><tr><td>eval/samples_per_second</td><td>2.674</td></tr><tr><td>eval/steps_per_second</td><td>0.675</td></tr><tr><td>eval/wer</td><td>41.78628</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>4.62660113793024e+18</td></tr><tr><td>train/train_loss</td><td>0.53709</td></tr><tr><td>train/train_runtime</td><td>884.6079</td></tr><tr><td>train/train_samples_per_second</td><td>4.522</td></tr><tr><td>train/train_steps_per_second</td><td>0.283</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">splendid-cherry-43</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/207uwibk' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/207uwibk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_185440-207uwibk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0dae971271a4452b17c37dbf0baf8f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112672066762268, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_190927-hanq372n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hanq372n' target=\"_blank\">copper-dream-44</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hanq372n' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hanq372n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:28, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.427182</td>\n",
              "      <td>53.110048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 19:24:09,923] Trial 43 pruned. \n",
            "Trial: {'learning_rate': 6.0354641999597654e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdeaaf1c3be14a1299317e4840a49655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.42718</td></tr><tr><td>eval/runtime</td><td>45.8218</td></tr><tr><td>eval/samples_per_second</td><td>2.248</td></tr><tr><td>eval/steps_per_second</td><td>0.567</td></tr><tr><td>eval/wer</td><td>53.11005</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">copper-dream-44</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hanq372n' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hanq372n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_190927-hanq372n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef1b97f105ab45619a339cd70150ceb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113842811028007, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_192417-ntyz1bhx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ntyz1bhx' target=\"_blank\">wobbly-donkey-45</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ntyz1bhx' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ntyz1bhx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.400564</td>\n",
              "      <td>41.148325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 19:38:54,485] Trial 44 finished with value: 41.14832535885167 and parameters: {'learning_rate': 6.0354641999597654e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 9.274824805648656e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07e02e2cd17f4a2c889818290e308706",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.097017…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40056</td></tr><tr><td>eval/runtime</td><td>38.5276</td></tr><tr><td>eval/samples_per_second</td><td>2.673</td></tr><tr><td>eval/steps_per_second</td><td>0.675</td></tr><tr><td>eval/wer</td><td>41.14833</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.31330056896512e+18</td></tr><tr><td>train/train_loss</td><td>0.54068</td></tr><tr><td>train/train_runtime</td><td>881.7738</td></tr><tr><td>train/train_samples_per_second</td><td>4.536</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wobbly-donkey-45</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ntyz1bhx' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ntyz1bhx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_192417-ntyz1bhx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46b9caeddf5b4f229f22d265444deddf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113977244369582, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_193902-vvdup3bc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vvdup3bc' target=\"_blank\">copper-firebrand-46</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vvdup3bc' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vvdup3bc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.460339</td>\n",
              "      <td>46.889952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 19:53:40,033] Trial 45 finished with value: 46.889952153110045 and parameters: {'learning_rate': 9.274824805648656e-06, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 5.090896319025339e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.46034</td></tr><tr><td>eval/runtime</td><td>38.8556</td></tr><tr><td>eval/samples_per_second</td><td>2.651</td></tr><tr><td>eval/steps_per_second</td><td>0.669</td></tr><tr><td>eval/wer</td><td>46.88995</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.87988</td></tr><tr><td>train/train_runtime</td><td>882.7514</td></tr><tr><td>train/train_samples_per_second</td><td>4.531</td></tr><tr><td>train/train_steps_per_second</td><td>0.283</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">copper-firebrand-46</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vvdup3bc' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/vvdup3bc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_193902-vvdup3bc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2aeff73c91af4283b53a5579eed21a3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112595644469063, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_195348-cze8dfli</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/cze8dfli' target=\"_blank\">logical-plasma-47</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/cze8dfli' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/cze8dfli</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:21, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.419460</td>\n",
              "      <td>41.945774</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 20:08:23,470] Trial 46 finished with value: 41.94577352472089 and parameters: {'learning_rate': 5.090896319025339e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 5.6049259790324566e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41946</td></tr><tr><td>eval/runtime</td><td>38.7146</td></tr><tr><td>eval/samples_per_second</td><td>2.66</td></tr><tr><td>eval/steps_per_second</td><td>0.672</td></tr><tr><td>eval/wer</td><td>41.94577</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53985</td></tr><tr><td>train/train_runtime</td><td>880.6027</td></tr><tr><td>train/train_samples_per_second</td><td>4.542</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">logical-plasma-47</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/cze8dfli' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/cze8dfli</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_195348-cze8dfli/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08931c17e3c94ff5bf3e8fc87241ffdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113648099934734, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_200836-byod0oz8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/byod0oz8' target=\"_blank\">breezy-wind-48</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/byod0oz8' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/byod0oz8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:33, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.415749</td>\n",
              "      <td>53.110048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 20:23:22,986] Trial 47 pruned. \n",
            "Trial: {'learning_rate': 3.3811900386153934e-07, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bf47888318543edaaa2ba060edafcce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41575</td></tr><tr><td>eval/runtime</td><td>46.2241</td></tr><tr><td>eval/samples_per_second</td><td>2.228</td></tr><tr><td>eval/steps_per_second</td><td>0.562</td></tr><tr><td>eval/wer</td><td>53.11005</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">breezy-wind-48</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/byod0oz8' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/byod0oz8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_200836-byod0oz8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2e9c05337549bf90ee45da7326254b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112721311090153, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_202330-rnny67f6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rnny67f6' target=\"_blank\">legendary-star-49</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rnny67f6' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rnny67f6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 16:18, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.776094</td>\n",
              "      <td>112.440191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 20:40:03,425] Trial 48 pruned. \n",
            "Trial: {'learning_rate': 4.125822862138353e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46cc64bc0c364e65ab70ea55b5abee5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.030 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.060273…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.77609</td></tr><tr><td>eval/runtime</td><td>160.5458</td></tr><tr><td>eval/samples_per_second</td><td>0.642</td></tr><tr><td>eval/steps_per_second</td><td>0.162</td></tr><tr><td>eval/wer</td><td>112.44019</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-star-49</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rnny67f6' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rnny67f6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_202330-rnny67f6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "636138407fac473a879d8f6e537529ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113670411254538, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_204011-loftss37</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/loftss37' target=\"_blank\">sunny-moon-50</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/loftss37' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/loftss37</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:21, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.408666</td>\n",
              "      <td>51.036683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 20:54:46,952] Trial 49 pruned. \n",
            "Trial: {'learning_rate': 1.9075437307174494e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "553803b78f1643a69862da3dfab734a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40867</td></tr><tr><td>eval/runtime</td><td>45.5538</td></tr><tr><td>eval/samples_per_second</td><td>2.261</td></tr><tr><td>eval/steps_per_second</td><td>0.571</td></tr><tr><td>eval/wer</td><td>51.03668</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sunny-moon-50</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/loftss37' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/loftss37</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_204011-loftss37/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e26f52093c47f0a28ae01844ffccd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111370254447037, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_205455-t293i221</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t293i221' target=\"_blank\">noble-cherry-51</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t293i221' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t293i221</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:09, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.424926</td>\n",
              "      <td>43.540670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 21:09:18,722] Trial 50 finished with value: 43.54066985645933 and parameters: {'learning_rate': 1.9075437307174494e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 1.9026368922579735e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39ce7cb7da904ecd9d02203cae082191",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.42493</td></tr><tr><td>eval/runtime</td><td>38.1899</td></tr><tr><td>eval/samples_per_second</td><td>2.697</td></tr><tr><td>eval/steps_per_second</td><td>0.681</td></tr><tr><td>eval/wer</td><td>43.54067</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>4.62660113793024e+18</td></tr><tr><td>train/train_loss</td><td>0.65847</td></tr><tr><td>train/train_runtime</td><td>868.9895</td></tr><tr><td>train/train_samples_per_second</td><td>4.603</td></tr><tr><td>train/train_steps_per_second</td><td>0.288</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-cherry-51</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t293i221' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/t293i221</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_205455-t293i221/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1b406d29f7c40b4a6659cac380cf3e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113472511189887, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_210926-hkkpriee</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hkkpriee' target=\"_blank\">treasured-lion-52</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hkkpriee' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hkkpriee</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:14, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.424996</td>\n",
              "      <td>43.062201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 21:23:54,033] Trial 51 finished with value: 43.0622009569378 and parameters: {'learning_rate': 1.9026368922579735e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 1.868865636383963e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.425</td></tr><tr><td>eval/runtime</td><td>38.5441</td></tr><tr><td>eval/samples_per_second</td><td>2.672</td></tr><tr><td>eval/steps_per_second</td><td>0.675</td></tr><tr><td>eval/wer</td><td>43.0622</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.65899</td></tr><tr><td>train/train_runtime</td><td>872.5181</td></tr><tr><td>train/train_samples_per_second</td><td>4.584</td></tr><tr><td>train/train_steps_per_second</td><td>0.287</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">treasured-lion-52</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hkkpriee' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/hkkpriee</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_210926-hkkpriee/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8edc3eb14a74b4788bcb03eecb387ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112542966697624, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_212400-djiu78gb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/djiu78gb' target=\"_blank\">flowing-silence-53</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/djiu78gb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/djiu78gb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:17, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.426784</td>\n",
              "      <td>53.110048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-19 21:38:31,646] Trial 52 pruned. \n",
            "Trial: {'learning_rate': 6.454584755233857e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.42678</td></tr><tr><td>eval/runtime</td><td>45.2255</td></tr><tr><td>eval/samples_per_second</td><td>2.277</td></tr><tr><td>eval/steps_per_second</td><td>0.575</td></tr><tr><td>eval/wer</td><td>53.11005</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">flowing-silence-53</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/djiu78gb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/djiu78gb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_212400-djiu78gb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ba90ac10d734467b767eefb61f6be7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111353129995728, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_213839-6jae085r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/6jae085r' target=\"_blank\">swept-firebrand-54</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/6jae085r' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/6jae085r</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:08, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.411442</td>\n",
              "      <td>42.902711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 21:53:01,582] Trial 53 finished with value: 42.90271132376395 and parameters: {'learning_rate': 6.454584755233857e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 6.934471066038605e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d175df76821947268a497fec06f9ec35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41144</td></tr><tr><td>eval/runtime</td><td>37.9939</td></tr><tr><td>eval/samples_per_second</td><td>2.711</td></tr><tr><td>eval/steps_per_second</td><td>0.684</td></tr><tr><td>eval/wer</td><td>42.90271</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.31330056896512e+18</td></tr><tr><td>train/train_loss</td><td>0.53764</td></tr><tr><td>train/train_runtime</td><td>867.1311</td></tr><tr><td>train/train_samples_per_second</td><td>4.613</td></tr><tr><td>train/train_steps_per_second</td><td>0.288</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-firebrand-54</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/6jae085r' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/6jae085r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_213839-6jae085r/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3f8c44714b6464bbe34f0fda7cd1881",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113378477845496, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_215308-c26bnxgm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/c26bnxgm' target=\"_blank\">devout-sponge-55</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/c26bnxgm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/c26bnxgm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.406563</td>\n",
              "      <td>42.902711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 22:07:45,936] Trial 54 finished with value: 42.90271132376395 and parameters: {'learning_rate': 6.934471066038605e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 7.35029578542228e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40656</td></tr><tr><td>eval/runtime</td><td>38.8168</td></tr><tr><td>eval/samples_per_second</td><td>2.653</td></tr><tr><td>eval/steps_per_second</td><td>0.67</td></tr><tr><td>eval/wer</td><td>42.90271</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53621</td></tr><tr><td>train/train_runtime</td><td>881.39</td></tr><tr><td>train/train_samples_per_second</td><td>4.538</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-sponge-55</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/c26bnxgm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/c26bnxgm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_215308-c26bnxgm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69d18abcca944172bcd288ea389dca17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113501855612008, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_220754-ilqm9i4x</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ilqm9i4x' target=\"_blank\">elated-sponge-56</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ilqm9i4x' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ilqm9i4x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:20, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.407944</td>\n",
              "      <td>41.148325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 22:22:28,690] Trial 55 finished with value: 41.14832535885167 and parameters: {'learning_rate': 7.35029578542228e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 7.332091448683441e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d18baac3e68c4584acfb194b834f6a4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.030 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.059341…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40794</td></tr><tr><td>eval/runtime</td><td>38.5574</td></tr><tr><td>eval/samples_per_second</td><td>2.671</td></tr><tr><td>eval/steps_per_second</td><td>0.674</td></tr><tr><td>eval/wer</td><td>41.14833</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53675</td></tr><tr><td>train/train_runtime</td><td>879.7654</td></tr><tr><td>train/train_samples_per_second</td><td>4.547</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">elated-sponge-56</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ilqm9i4x' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/ilqm9i4x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_220754-ilqm9i4x/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "144684f106d94cc4a68b4109d39e37be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111276114453277, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_222235-rbwgbvf1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rbwgbvf1' target=\"_blank\">light-bee-57</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rbwgbvf1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rbwgbvf1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:19, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.402636</td>\n",
              "      <td>40.031898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-19 22:37:08,402] Trial 56 finished with value: 40.03189792663477 and parameters: {'learning_rate': 7.332091448683441e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 9.546583272270006e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2941687390884e79b6256a144799036a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.019 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.094550…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40264</td></tr><tr><td>eval/runtime</td><td>38.6922</td></tr><tr><td>eval/samples_per_second</td><td>2.662</td></tr><tr><td>eval/steps_per_second</td><td>0.672</td></tr><tr><td>eval/wer</td><td>40.0319</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53584</td></tr><tr><td>train/train_runtime</td><td>876.8843</td></tr><tr><td>train/train_samples_per_second</td><td>4.562</td></tr><tr><td>train/train_steps_per_second</td><td>0.285</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light-bee-57</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rbwgbvf1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/rbwgbvf1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230919_222235-rbwgbvf1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b7dce3966e14df1a4be9d251c49bd81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112527666652265, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230919_223715-bv6qooth</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bv6qooth' target=\"_blank\">leafy-sunset-58</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bv6qooth' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/bv6qooth</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 49/250 02:35 < 11:02, 0.30 it/s, Epoch 0.28/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 04:46:24,305] Trial 83 finished with value: 41.94577352472089 and parameters: {'learning_rate': 4.99610136837039e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 7.96257436274761e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52ef0e1eb6f4458ba36d22574bf1ae9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.096980…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41782</td></tr><tr><td>eval/runtime</td><td>38.2333</td></tr><tr><td>eval/samples_per_second</td><td>2.694</td></tr><tr><td>eval/steps_per_second</td><td>0.68</td></tr><tr><td>eval/wer</td><td>41.94577</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.54175</td></tr><tr><td>train/train_runtime</td><td>867.6949</td></tr><tr><td>train/train_samples_per_second</td><td>4.61</td></tr><tr><td>train/train_steps_per_second</td><td>0.288</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">driven-flower-84</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/16rcwgwi' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/16rcwgwi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_043201-16rcwgwi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9612e63ed7a54b76848a833b4e9901ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112656099915815, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_044632-3lq6fflv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3lq6fflv' target=\"_blank\">usual-rain-85</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3lq6fflv' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3lq6fflv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:12, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.423746</td>\n",
              "      <td>41.786284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 05:00:58,816] Trial 84 finished with value: 41.78628389154705 and parameters: {'learning_rate': 7.96257436274761e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 5.5111105079737786e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d58ec90e1d094a04960765c8c829c0ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.42375</td></tr><tr><td>eval/runtime</td><td>38.1964</td></tr><tr><td>eval/samples_per_second</td><td>2.697</td></tr><tr><td>eval/steps_per_second</td><td>0.681</td></tr><tr><td>eval/wer</td><td>41.78628</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53882</td></tr><tr><td>train/train_runtime</td><td>871.6326</td></tr><tr><td>train/train_samples_per_second</td><td>4.589</td></tr><tr><td>train/train_steps_per_second</td><td>0.287</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-rain-85</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3lq6fflv' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/3lq6fflv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_044632-3lq6fflv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c07d419e6c644401b62523a593482442",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113581611102239, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_050105-qht4shx1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/qht4shx1' target=\"_blank\">easy-terrain-86</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/qht4shx1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/qht4shx1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:07, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.413019</td>\n",
              "      <td>43.859649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 05:15:27,630] Trial 85 finished with value: 43.859649122807014 and parameters: {'learning_rate': 5.5111105079737786e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 2.2073850750759435e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c9c2d38e24046ceaf5222c2df4853ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41302</td></tr><tr><td>eval/runtime</td><td>38.0795</td></tr><tr><td>eval/samples_per_second</td><td>2.705</td></tr><tr><td>eval/steps_per_second</td><td>0.683</td></tr><tr><td>eval/wer</td><td>43.85965</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.53788</td></tr><tr><td>train/train_runtime</td><td>865.9794</td></tr><tr><td>train/train_samples_per_second</td><td>4.619</td></tr><tr><td>train/train_steps_per_second</td><td>0.289</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">easy-terrain-86</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/qht4shx1' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/qht4shx1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_050105-qht4shx1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba721b75b2f7484f90b8b6e6b9ecd62b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111135297556757, max=1.0))…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_051535-m0pk157b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/m0pk157b' target=\"_blank\">divine-fire-87</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/m0pk157b' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/m0pk157b</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:14, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.418698</td>\n",
              "      <td>51.196172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 05:30:04,136] Trial 86 pruned. \n",
            "Trial: {'learning_rate': 1.5804396803323326e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5539da0deeae4a6d98e1ec918b34ed90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.4187</td></tr><tr><td>eval/runtime</td><td>45.3311</td></tr><tr><td>eval/samples_per_second</td><td>2.272</td></tr><tr><td>eval/steps_per_second</td><td>0.574</td></tr><tr><td>eval/wer</td><td>51.19617</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-fire-87</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/m0pk157b' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/m0pk157b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_051535-m0pk157b/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebc8a4f454414f73b06b9fede74437e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114361299971481, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_053011-fereapec</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fereapec' target=\"_blank\">morning-dawn-88</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fereapec' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fereapec</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:19, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.431553</td>\n",
              "      <td>53.748006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 05:44:44,886] Trial 87 pruned. \n",
            "Trial: {'learning_rate': 3.568014316961694e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 2}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "751521b374b74c049696953f082fa4c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.43155</td></tr><tr><td>eval/runtime</td><td>45.5006</td></tr><tr><td>eval/samples_per_second</td><td>2.264</td></tr><tr><td>eval/steps_per_second</td><td>0.571</td></tr><tr><td>eval/wer</td><td>53.74801</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">morning-dawn-88</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fereapec' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/fereapec</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_053011-fereapec/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbb78e82883c41cbab2933044c6e1665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113316266629328, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_054452-b645z1xb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/b645z1xb' target=\"_blank\">good-smoke-89</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/b645z1xb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/b645z1xb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:21, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.405809</td>\n",
              "      <td>51.674641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 2\n",
            "[I 2023-09-20 05:59:27,789] Trial 88 pruned. \n",
            "Trial: {'learning_rate': 8.228384509215973e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2a0fd74877407e9bc01966b44898d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40581</td></tr><tr><td>eval/runtime</td><td>45.1977</td></tr><tr><td>eval/samples_per_second</td><td>2.279</td></tr><tr><td>eval/steps_per_second</td><td>1.151</td></tr><tr><td>eval/wer</td><td>51.67464</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-smoke-89</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/b645z1xb' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/b645z1xb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_054452-b645z1xb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c2da2b011d7448d8cb462a31bec135b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113421477784869, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_055936-zgpal3jm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/zgpal3jm' target=\"_blank\">hardy-forest-90</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/zgpal3jm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/zgpal3jm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:22, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.417497</td>\n",
              "      <td>41.148325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 06:14:12,513] Trial 89 finished with value: 41.14832535885167 and parameters: {'learning_rate': 8.228384509215973e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 7.800009412894888e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.4175</td></tr><tr><td>eval/runtime</td><td>39.0234</td></tr><tr><td>eval/samples_per_second</td><td>2.639</td></tr><tr><td>eval/steps_per_second</td><td>0.666</td></tr><tr><td>eval/wer</td><td>41.14833</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>4.62660113793024e+18</td></tr><tr><td>train/train_loss</td><td>0.53531</td></tr><tr><td>train/train_runtime</td><td>881.7036</td></tr><tr><td>train/train_samples_per_second</td><td>4.537</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hardy-forest-90</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/zgpal3jm' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/zgpal3jm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_055936-zgpal3jm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0425cd87051486dbd4e5e82ca1cedb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113254811142623, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_061421-s33bui8t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/s33bui8t' target=\"_blank\">rich-water-91</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/s33bui8t' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/s33bui8t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:35, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.427138</td>\n",
              "      <td>52.631579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 06:29:11,853] Trial 90 pruned. \n",
            "Trial: {'learning_rate': 5.599048443476005e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f633a616b5f5443d9df4610613c78462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.035 MB of 0.035 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.42714</td></tr><tr><td>eval/runtime</td><td>46.6186</td></tr><tr><td>eval/samples_per_second</td><td>2.209</td></tr><tr><td>eval/steps_per_second</td><td>0.558</td></tr><tr><td>eval/wer</td><td>52.63158</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-water-91</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/s33bui8t' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/s33bui8t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_061421-s33bui8t/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d93b625aa154a5fb96fa1dbf7e45787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113875044490367, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_062917-xmjx4oex</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xmjx4oex' target=\"_blank\">copper-wildflower-92</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xmjx4oex' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xmjx4oex</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:26, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.408336</td>\n",
              "      <td>43.381180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 06:43:58,414] Trial 91 finished with value: 43.38118022328549 and parameters: {'learning_rate': 5.599048443476005e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 9.815172119812488e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40834</td></tr><tr><td>eval/runtime</td><td>38.9196</td></tr><tr><td>eval/samples_per_second</td><td>2.646</td></tr><tr><td>eval/steps_per_second</td><td>0.668</td></tr><tr><td>eval/wer</td><td>43.38118</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.31330056896512e+18</td></tr><tr><td>train/train_loss</td><td>0.53999</td></tr><tr><td>train/train_runtime</td><td>883.6978</td></tr><tr><td>train/train_samples_per_second</td><td>4.526</td></tr><tr><td>train/train_steps_per_second</td><td>0.283</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">copper-wildflower-92</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xmjx4oex' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/xmjx4oex</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_062917-xmjx4oex/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4118db1660a4bb7a854872c99d8cc8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113629166761205, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_064405-487rpr6i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/487rpr6i' target=\"_blank\">rosy-gorge-93</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/487rpr6i' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/487rpr6i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:24, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.471863</td>\n",
              "      <td>44.657097</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 06:58:44,946] Trial 92 pruned. \n",
            "Trial: {'learning_rate': 4.5475975604118014e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.47186</td></tr><tr><td>eval/runtime</td><td>38.9581</td></tr><tr><td>eval/samples_per_second</td><td>2.644</td></tr><tr><td>eval/steps_per_second</td><td>0.667</td></tr><tr><td>eval/wer</td><td>44.6571</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rosy-gorge-93</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/487rpr6i' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/487rpr6i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_064405-487rpr6i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01c580d242bf4d72ae17946716d2bcfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113661744457205, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_065853-k73o4q6a</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/k73o4q6a' target=\"_blank\">balmy-monkey-94</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/k73o4q6a' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/k73o4q6a</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:35, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.402014</td>\n",
              "      <td>51.036683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 07:13:43,112] Trial 93 pruned. \n",
            "Trial: {'learning_rate': 7.655754305676462e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89600ebbaf4a46b6bdef3eccb9d403d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40201</td></tr><tr><td>eval/runtime</td><td>46.2286</td></tr><tr><td>eval/samples_per_second</td><td>2.228</td></tr><tr><td>eval/steps_per_second</td><td>0.562</td></tr><tr><td>eval/wer</td><td>51.03668</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">balmy-monkey-94</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/k73o4q6a' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/k73o4q6a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_065853-k73o4q6a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcea2c49df394e16a8d7ba75c72fd830",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113779555631077, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_071349-0rlcpyc0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0rlcpyc0' target=\"_blank\">jolly-violet-95</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0rlcpyc0' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0rlcpyc0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:25, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.412439</td>\n",
              "      <td>43.540670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 07:28:29,336] Trial 94 finished with value: 43.54066985645933 and parameters: {'learning_rate': 7.655754305676462e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 3.112099508511964e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44162117d531444fa06a4534dc83d1f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.097022…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41244</td></tr><tr><td>eval/runtime</td><td>38.6811</td></tr><tr><td>eval/samples_per_second</td><td>2.663</td></tr><tr><td>eval/steps_per_second</td><td>0.672</td></tr><tr><td>eval/wer</td><td>43.54067</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>3.46995085344768e+18</td></tr><tr><td>train/train_loss</td><td>0.53615</td></tr><tr><td>train/train_runtime</td><td>883.3946</td></tr><tr><td>train/train_samples_per_second</td><td>4.528</td></tr><tr><td>train/train_steps_per_second</td><td>0.283</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jolly-violet-95</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0rlcpyc0' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/0rlcpyc0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_071349-0rlcpyc0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e0a3bfa560a45869b11e911ee7ce5de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114388499926362, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_072836-sa7fzsa3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/sa7fzsa3' target=\"_blank\">honest-puddle-96</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/sa7fzsa3' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/sa7fzsa3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:33, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.405480</td>\n",
              "      <td>42.902711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 07:43:24,308] Trial 95 finished with value: 42.90271132376395 and parameters: {'learning_rate': 3.112099508511964e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 6.101233758703358e-05, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcc0559546ba4ec58c6ab5b2b115163e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.40548</td></tr><tr><td>eval/runtime</td><td>39.3235</td></tr><tr><td>eval/samples_per_second</td><td>2.619</td></tr><tr><td>eval/steps_per_second</td><td>0.661</td></tr><tr><td>eval/wer</td><td>42.90271</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>1.15665028448256e+18</td></tr><tr><td>train/train_loss</td><td>0.5835</td></tr><tr><td>train/train_runtime</td><td>892.136</td></tr><tr><td>train/train_samples_per_second</td><td>4.484</td></tr><tr><td>train/train_steps_per_second</td><td>0.28</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">honest-puddle-96</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/sa7fzsa3' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/sa7fzsa3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_072836-sa7fzsa3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f6f85d360b54143b907c5204f15261e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113875211028952, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_074333-iugss8g7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/iugss8g7' target=\"_blank\">ruby-sun-97</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/iugss8g7' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/iugss8g7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 04:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.563064</td>\n",
              "      <td>54.385965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 07:48:31,869] Trial 96 pruned. \n",
            "Trial: {'learning_rate': 2.280349231895939e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.56306</td></tr><tr><td>eval/runtime</td><td>39.1197</td></tr><tr><td>eval/samples_per_second</td><td>2.633</td></tr><tr><td>eval/steps_per_second</td><td>0.665</td></tr><tr><td>eval/wer</td><td>54.38596</td></tr><tr><td>train/epoch</td><td>0.37</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ruby-sun-97</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/iugss8g7' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/iugss8g7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_074333-iugss8g7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13ef515ecce8409993e1b526429f9342",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114172844423188, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_074840-y7dcpeue</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/y7dcpeue' target=\"_blank\">stoic-disco-98</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/y7dcpeue' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/y7dcpeue</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:39, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.412865</td>\n",
              "      <td>51.515152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "[I 2023-09-20 08:03:33,362] Trial 97 pruned. \n",
            "Trial: {'learning_rate': 3.997730302981051e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee18e78f9bb44fac9543258210f571b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.152737…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.41287</td></tr><tr><td>eval/runtime</td><td>46.2628</td></tr><tr><td>eval/samples_per_second</td><td>2.226</td></tr><tr><td>eval/steps_per_second</td><td>0.562</td></tr><tr><td>eval/wer</td><td>51.51515</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stoic-disco-98</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/y7dcpeue' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/y7dcpeue</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_074840-y7dcpeue/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0faf7a82adf94091865329547dff0e2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113854800027589, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_080342-jl65dj5u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jl65dj5u' target=\"_blank\">clear-snowflake-99</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jl65dj5u' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jl65dj5u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:20, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.413402</td>\n",
              "      <td>41.786284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 08:18:16,877] Trial 98 finished with value: 41.78628389154705 and parameters: {'learning_rate': 3.997730302981051e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n",
            "Trial: {'learning_rate': 6.027685225621409e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}\n",
            "loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/config.json\n",
            "Model config WhisperConfig {\n",
            "  \"_name_or_path\": \"openai/whisper-small\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"WhisperForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"dropout\": 0.0,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 448,\n",
            "  \"max_source_positions\": 1500,\n",
            "  \"max_target_positions\": 448,\n",
            "  \"model_type\": \"whisper\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_mel_bins\": 80,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"scale_embedding\": false,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 51865\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      50259\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ],\n",
            "    [\n",
            "      3,\n",
            "      50363\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--openai--whisper-small/snapshots/e34e8ae444c29815eca53e11383ea13b2e362eb0/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    220,\n",
            "    50257\n",
            "  ],\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"decoder_start_token_id\": 50258,\n",
            "  \"eos_token_id\": 50257,\n",
            "  \"forced_decoder_ids\": [\n",
            "    [\n",
            "      1,\n",
            "      null\n",
            "    ],\n",
            "    [\n",
            "      2,\n",
            "      50359\n",
            "    ]\n",
            "  ],\n",
            "  \"max_length\": 448,\n",
            "  \"pad_token_id\": 50257,\n",
            "  \"suppress_tokens\": [\n",
            "    1,\n",
            "    2,\n",
            "    7,\n",
            "    8,\n",
            "    9,\n",
            "    10,\n",
            "    14,\n",
            "    25,\n",
            "    26,\n",
            "    27,\n",
            "    28,\n",
            "    29,\n",
            "    31,\n",
            "    58,\n",
            "    59,\n",
            "    60,\n",
            "    61,\n",
            "    62,\n",
            "    63,\n",
            "    90,\n",
            "    91,\n",
            "    92,\n",
            "    93,\n",
            "    359,\n",
            "    503,\n",
            "    522,\n",
            "    542,\n",
            "    873,\n",
            "    893,\n",
            "    902,\n",
            "    918,\n",
            "    922,\n",
            "    931,\n",
            "    1350,\n",
            "    1853,\n",
            "    1982,\n",
            "    2460,\n",
            "    2627,\n",
            "    3246,\n",
            "    3253,\n",
            "    3268,\n",
            "    3536,\n",
            "    3846,\n",
            "    3961,\n",
            "    4183,\n",
            "    4667,\n",
            "    6585,\n",
            "    6647,\n",
            "    7273,\n",
            "    9061,\n",
            "    9383,\n",
            "    10428,\n",
            "    10929,\n",
            "    11938,\n",
            "    12033,\n",
            "    12331,\n",
            "    12562,\n",
            "    13793,\n",
            "    14157,\n",
            "    14635,\n",
            "    15265,\n",
            "    15618,\n",
            "    16553,\n",
            "    16604,\n",
            "    18362,\n",
            "    18956,\n",
            "    20075,\n",
            "    21675,\n",
            "    22520,\n",
            "    26130,\n",
            "    26161,\n",
            "    26435,\n",
            "    28279,\n",
            "    29464,\n",
            "    31650,\n",
            "    32302,\n",
            "    32470,\n",
            "    36865,\n",
            "    42863,\n",
            "    47425,\n",
            "    49870,\n",
            "    50254,\n",
            "    50258,\n",
            "    50358,\n",
            "    50359,\n",
            "    50360,\n",
            "    50361,\n",
            "    50362\n",
            "  ],\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "***** Running training *****\n",
            "  Num examples = 2696\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 250\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval/wer</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.4134</td></tr><tr><td>eval/runtime</td><td>38.8375</td></tr><tr><td>eval/samples_per_second</td><td>2.652</td></tr><tr><td>eval/steps_per_second</td><td>0.669</td></tr><tr><td>eval/wer</td><td>41.78628</td></tr><tr><td>train/epoch</td><td>1.49</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/total_flos</td><td>2.60188596928512e+18</td></tr><tr><td>train/train_loss</td><td>0.55599</td></tr><tr><td>train/train_runtime</td><td>880.7003</td></tr><tr><td>train/train_samples_per_second</td><td>4.542</td></tr><tr><td>train/train_steps_per_second</td><td>0.284</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clear-snowflake-99</strong> at: <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jl65dj5u' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/jl65dj5u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230920_080342-jl65dj5u/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9606a4e5ec2d46a68ae59e08c5a51923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113109389003107, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/wandb/run-20230920_081825-kt82fity</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kt82fity' target=\"_blank\">confused-fog-100</a></strong> to <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kt82fity' target=\"_blank\">https://wandb.ai/mohammadh/hyperparameter_tuning_whisper_small_persian_data_11/runs/kt82fity</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:30, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.394662</td>\n",
              "      <td>40.988836</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 103\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[I 2023-09-20 08:33:09,372] Trial 99 finished with value: 40.98883572567783 and parameters: {'learning_rate': 6.027685225621409e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4}. Best is trial 36 with value: 39.71291866028708.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BestRun(run_id='36', objective=39.71291866028708, hyperparameters={'learning_rate': 6.150443574387932e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4})\n"
          ]
        }
      ],
      "source": [
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-9, 1e-4, log=True),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8]),\n",
        "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [2, 4, 8]),\n",
        "    }\n",
        "\n",
        "best_run = trainer.hyperparameter_search(hp_space=my_hp_space, n_trials=100, direction=\"minimize\")\n",
        "print(best_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJJ-Ljb7OLTI",
        "outputId": "5db1e61b-41d0-4723-b3c8-2f84531377b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BestRun(run_id='36', objective=39.71291866028708, hyperparameters={'learning_rate': 6.150443574387932e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4})\n"
          ]
        }
      ],
      "source": [
        "print(best_run)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-env-pytorch-pytorch",
      "name": "workbench-notebooks.m111",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
    },
    "kernelspec": {
      "display_name": "PyTorch 1-13",
      "language": "python",
      "name": "conda-env-pytorch-pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}